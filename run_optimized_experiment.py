# ==========================================
# 1. ç³»çµ±è¨­å®š (å®‰éœæ¨¡å¼)
# ==========================================
import sys
import os
import warnings
import logging
import inspect
from pathlib import Path

logging.disable(logging.CRITICAL)

class AggressiveFilter:
    def __init__(self, original_stream):
        self.original_stream = original_stream
    def write(self, message):
        block_keywords = [">>>> Params", "prediction.classifier", "feature_scaling_candidate", "preprocessor.dimensionality"]
        if any(k in message for k in block_keywords): return
        self.original_stream.write(message)
    def flush(self):
        try: self.original_stream.flush()
        except: pass

# ==========================================
# 2. è¼‰å…¥å¥—ä»¶
# ==========================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, 
    recall_score, confusion_matrix, roc_auc_score, 
    roc_curve, auc
)

current_dir = os.getcwd()
src_dir = os.path.join(current_dir, "src")
if src_dir not in sys.path: sys.path.insert(0, src_dir)

try:
    from autoprognosis.studies.classifiers import ClassifierStudy
    from autoprognosis.utils.serialization import save_model_to_file, load_model_from_file
    from autoprognosis.plugins.prediction.classifiers import Classifiers
except ImportError:
    print("âŒ æ‰¾ä¸åˆ° AutoPrognosisï¼Œè«‹ç¢ºèªè·¯å¾‘ã€‚"); sys.exit(1)

warnings.filterwarnings("ignore")
os.environ['PYTHONWARNINGS'] = 'ignore'
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei']
plt.rcParams['axes.unicode_minus'] = False
sns.set(font='Microsoft JhengHei')
save_dir = os.getcwd()

# ==========================================
# 3. [æ ¸ç£å…±æŒ¯] ç‰©ä»¶æ·±åº¦æœå°‹å¼•æ“
# ==========================================

def get_simple_name(obj):
    """å¾ç‰©ä»¶åç¨±çŒœæ¼”ç®—æ³•"""
    raw = type(obj).__name__.lower()
    if "xgboost" in raw: return "XGBoost"
    if "catboost" in raw: return "CatBoost"
    if "forest" in raw: return "Random Forest"
    if "svm" in raw or "svc" in raw: return "Linear SVM"
    if "logistic" in raw: return "Logistic Regression"
    return "Unknown Model"

def recursive_search(obj, target_keys, found_repo, visited, path=""):
    """
    éè¿´æƒæç‰©ä»¶çš„æ‰€æœ‰å±¬æ€§ï¼Œå°‹æ‰¾ç›®æ¨™åƒæ•¸
    """
    # é˜²æ­¢ç„¡çª®è¿´åœˆ
    obj_id = id(obj)
    if obj_id in visited:
        return
    visited.add(obj_id)
    
    # 1. æª¢æŸ¥è‡ªå·±æ˜¯å¦æ˜¯ç›®æ¨™
    # å¦‚æœ obj æœ¬èº«å°±æ˜¯ä¸€å€‹å­—å…¸ (ä¾‹å¦‚ hyperparameters={...})
    if isinstance(obj, dict):
        for k, v in obj.items():
            if k in target_keys and v is not None:
                # æ‰¾åˆ°å¯¶è—ï¼å­˜èµ·ä¾†
                if k not in found_repo: 
                    found_repo[k] = v
            # ç¹¼çºŒå¾€ä¸‹é‘½
            if hasattr(v, '__dict__') or isinstance(v, (dict, list)):
                recursive_search(v, target_keys, found_repo, visited, path + f".{k}")
        return

    # 2. æª¢æŸ¥ç‰©ä»¶çš„å±¬æ€§ (__dict__)
    if hasattr(obj, "__dict__"):
        for k, v in obj.__dict__.items():
            # å‘½ä¸­ç›®æ¨™ Key (ä¾‹å¦‚ self.C = 1.0)
            if k in target_keys and v is not None:
                if k not in found_repo:
                    found_repo[k] = v
            
            # æ’é™¤ç§æœ‰å±¬æ€§å’Œç³»çµ±å±¬æ€§ï¼Œå„ªåŒ–é€Ÿåº¦
            if k.startswith("__") or k in ['_logger', 'logger']:
                continue
                
            # ç¹¼çºŒå¾€ä¸‹é‘½ (éè¿´)
            if hasattr(v, '__dict__') or isinstance(v, (dict, list)):
                recursive_search(v, target_keys, found_repo, visited, path + f".{k}")

    # 3. ç‰¹åˆ¥è™•ç† List (ä¾‹å¦‚ steps=[...])
    if isinstance(obj, (list, tuple)):
        for item in obj:
             recursive_search(item, target_keys, found_repo, visited, path + "[]")

def deep_extract_params(model_obj):
    """
    å•Ÿå‹•æ·±åº¦æœå°‹ï¼Œé‡å°å–®ä¸€æ¨¡å‹ç‰©ä»¶
    """
    # å®šç¾©æˆ‘å€‘ä¸€å®šè¦æ‰¾åˆ°çš„é»ƒé‡‘åƒæ•¸
    target_keys = [
        'C', 'kernel', 'gamma',                      # SVM / LR
        'n_estimators', 'max_depth', 'learning_rate', # Trees
        'min_child_weight', 'reg_alpha', 'reg_lambda', # XGB
        'penalty', 'solver', 'l1_ratio',             # LR
        'n_components'                               # PCA
    ]
    
    found_params = {}
    visited = set()
    
    # é–‹å§‹åœ°æ¯¯å¼æœç´¢
    recursive_search(model_obj, target_keys, found_params, visited)
    
    # æ ¼å¼åŒ–è¼¸å‡º
    result_list = []
    for k, v in found_params.items():
        val_str = f"{v:.6g}" if isinstance(v, (float, int)) and not isinstance(v, bool) else f"{v}"
        result_list.append(f"{k} = {val_str}")
        
    return sorted(result_list)

def print_model_details(model):
    print("\n" + "="*80 + "\nğŸ”¬ æ¨¡å‹å…§éƒ¨åƒæ•¸æ·±åº¦æƒæå ±å‘Š (Deep Scan)\n" + "="*80)
    print("ğŸ’¡ èªªæ˜ï¼šæ­¤ç¨‹å¼å·²æš´åŠ›æƒæè¨˜æ†¶é«”ä¸­æ‰€æœ‰è®Šæ•¸ï¼Œä»¥ä¸‹æ˜¯æ‰¾åˆ°çš„é—œéµåƒæ•¸ã€‚\n")
    
    if hasattr(model, "models") and hasattr(model, "weights"):
        print("ã€æª¢æ¸¬çµæœã€‘ï¼šé›†æˆæ¨¡å‹ (Ensemble)")
        
        combined = []
        for m, w in zip(model.models, model.weights):
            # å¾ç‰©ä»¶åç¨±çŒœæ˜¯èª°
            real_obj = m.model if hasattr(m, "model") else m
            name = get_simple_name(real_obj)
            
            # å•Ÿå‹•æƒæ
            params = deep_extract_params(real_obj)
            combined.append({'name': name, 'weight': w, 'params': params})
            
        combined = sorted(combined, key=lambda x: x['weight'], reverse=True)
        ranks = ["ğŸ¥‡ ç¬¬ä¸€å", "ğŸ¥ˆ ç¬¬äºŒå", "ğŸ¥‰ ç¬¬ä¸‰å"]
        
        for i in range(min(3, len(combined))):
            comp = combined[i]
            print(f"{ranks[i]}ï¼š{comp['name']} (æ¬Šé‡: {comp['weight']*100:.2f}%)")
            print("-" * 80)
            
            if comp['params']:
                for line in comp['params']:
                    print(f"   ğŸ‘‰ {line}")
            else:
                print("   (âš ï¸ æƒæå®Œç•¢ä½†æœªç™¼ç¾ç›®æ¨™åƒæ•¸ï¼Œå¯èƒ½æ˜¯ä½¿ç”¨é è¨­å€¼æˆ–åç¨±è¢«æ··æ·†)")
            print("\n")
    else:
        print("ã€æª¢æ¸¬çµæœã€‘ï¼šå–®ä¸€æ¨¡å‹ (Single Algorithm)")
        real_obj = model
        name = get_simple_name(real_obj)
        params = deep_extract_params(real_obj)
        
        print(f"ğŸ¥‡ å”¯ä¸€å† è»ï¼š{name}")
        print("-" * 80)
        for line in params:
            print(f"   ğŸ‘‰ {line}")

# ==========================================
# 4. ç¹ªåœ–èˆ‡å…¶ä»–é‚è¼¯ (ä¸è®Š)
# ==========================================
def analyze_and_save_chart(y_true, y_pred, y_proba, title):
    metrics = {
        "Accuracy": accuracy_score(y_true, y_pred),
        "Precision": precision_score(y_true, y_pred, average='weighted'),
        "Recall": recall_score(y_true, y_pred, average='weighted'),
        "F1 Score": f1_score(y_true, y_pred, average='weighted'),
    }
    if y_proba is not None: metrics["AUC-ROC"] = roc_auc_score(y_true, y_proba)

    print(f"\n========================================\nğŸ† {title}\n========================================")
    for name, value in metrics.items(): print(f"{name:<15} : {value:.4f}")
    print("-" * 40)

    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])
    axes[0].set_title(f"Confusion Matrix\n{title}")
    
    if y_proba is not None:
        fpr, tpr, _ = roc_curve(y_true, y_proba)
        roc_auc = auc(fpr, tpr)
        axes[1].plot(fpr, tpr, color='orange', label=f'AUC = {roc_auc:.4f}')
        axes[1].plot([0, 1], [0, 1], '--')
        axes[1].set_title(f"ROC Curve\n{title}")
        axes[1].legend(loc="lower right")
    
    plt.tight_layout()
    filename = f"chart_{title.replace(' ', '_')}.png"
    plt.savefig(os.path.join(save_dir, filename))
    print(f"ğŸ“Š åœ–è¡¨å·²å­˜è‡³: {filename}")
    plt.close()

def perform_dry_run(train_df, target_col, classifiers, feature_selection):
    print("\nğŸ”¥ [2/2] å†’ç…™æ¸¬è©¦ (Dry Run)...")
    sample_size = min(100, int(len(train_df) * 0.5))
    small_df = train_df.sample(n=sample_size, random_state=42)
    if small_df[target_col].nunique() < 2:
        small_df = pd.concat([small_df, train_df.iloc[:5]]) 

    try:
        test_study = ClassifierStudy(
            study_name="dry_run", dataset=small_df, target=target_col,
            num_iter=1, timeout=60, classifiers=[classifiers[0]], metric="aucroc",
            feature_selection=[feature_selection[0]], score_threshold=0.0 
        )
        if test_study.fit() is None: raise RuntimeError("None Model")
        print("âœ… é€šé")
    except Exception as e:
        sys.stdout = sys.__stdout__; sys.stderr = sys.__stderr__
        print(f"\nâŒ æ¸¬è©¦å¤±æ•—: {e}"); raise e

def main():
    # ==========================================
    # âš™ï¸ è¨­å®šå€
    # ==========================================
    FORCE_RETRAIN = False 
    
    print("="*60)
    print(f"   AutoPrognosis ç³–å°¿ç—…é æ¸¬ (æ ¸ç£å…±æŒ¯ç‰ˆ)")
    print("="*60)
    
    file_path = r"C:\Users\aa803\Downloads\archive\diabetes.csv"
    if not os.path.exists(file_path): print(f"âŒ æ‰¾ä¸åˆ°: {file_path}"); return

    df = pd.read_csv(file_path)
    target = df.columns[8]
    train_df, test_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df[target])
    
    model_filename = "diabetes_best_model.p"
    model_path = Path(model_filename)
    model = None

    if model_path.exists() and not FORCE_RETRAIN:
        print(f"\nğŸ“‚ ç™¼ç¾æ¨¡å‹ï¼š{model_filename}")
        print("â© ç›´æ¥è¼‰å…¥...")
        model = load_model_from_file(model_path)
        print("âœ… è¼‰å…¥æˆåŠŸï¼")
    else:
        if FORCE_RETRAIN: print(f"\nâš ï¸  å·²é–‹å•Ÿå¼·åˆ¶é‡ç·´...")
        else: print(f"\nğŸš€ æœªç™¼ç¾æ¨¡å‹ï¼Œæº–å‚™è¨“ç·´...")
            
        classifiers = ["xgboost", "random_forest", "logistic_regression", "catboost", "linear_svm"]
        valid_classifiers = [c for c in classifiers if c in Classifiers().list_available()]
        feature_selection = ["pca", "variance_threshold", "nop"]

        perform_dry_run(train_df, target, valid_classifiers, feature_selection)
        
        sys.stdout = AggressiveFilter(sys.stdout)
        sys.stderr = AggressiveFilter(sys.stderr)
        
        print("\nğŸš€ [æ­£å¼é–‹å§‹] AutoML æœå°‹ (ç´„ 10 åˆ†é˜)...")
        TOTAL_TIME_LIMIT = 600
        time_per = int(TOTAL_TIME_LIMIT / len(valid_classifiers))
        
        study = ClassifierStudy(
            study_name="diabetes_run", dataset=train_df, target=target,
            num_iter=20, timeout=time_per, classifiers=valid_classifiers,
            metric="aucroc", feature_selection=feature_selection 
        )
        model = study.fit()
        
        sys.stdout = sys.__stdout__
        sys.stderr = sys.__stderr__
        
        if model is None: print("âŒ æœå°‹å¤±æ•—ã€‚"); return
        save_model_to_file(model_path, model)
        print("\nğŸ’¾ æ¨¡å‹å·²å„²å­˜ã€‚")

    X_test = test_df.drop(columns=[target])
    y_test = test_df[target]
    y_pred = model.predict(X_test).to_numpy().ravel()
    try: y_proba = model.predict_proba(X_test).to_numpy()[:, 1]
    except: y_proba = None

    analyze_and_save_chart(y_test, y_pred, y_proba, "æœ€ä½³æ¨¡å‹é©—è­‰")
    print_model_details(model)

if __name__ == "__main__":
    main()